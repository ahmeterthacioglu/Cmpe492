# -*- coding: utf-8 -*-
"""gemma_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19BX8X1I_i9ZOynMQK8LrsYFOZZm-UbaI
"""

pip install transformers torch pandas

import pandas as pd
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# Log in to Hugging Face
login(token="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")

# Load the tokenizer and model
model_id = "google/gemma-2-2b-it"
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=dtype,
)


print("Responses generated and saved to 'responses_only_choices.csv'")

def extract_numeric_response(output_text):
    """
    Extract the numeric response (e.g., '1', '2') from the output text.
    """
    import re
    # Match a single digit number
    match = re.match(r'^\s*(\d+)\s*$', output_text.strip())
    return match.group(1) if match else "(Yanıt bulunamadı)"
def extract_response_options(prompt):
    # Use regex to extract lines that start with a number followed by a period
    options = re.findall(r'^(\d+)\.\s', prompt, re.MULTILINE)
    return options
def ensure_valid_response(output_text, response_options):
    """Ensure the LLM's output matches one of the valid response options."""
    for option in response_options:
        if option in output_text:
            return option
    return "(Yanıt bulunamadı5)"  # Fallback if no valid response is found

def validate_response(output_text):
    """Extract the part after the number and period."""
    match = re.match(r'^[1-4]\.?\s*(.+)', output_text)
    if match:
        return match.group(1).strip()
    else:
        return "(Yanıt bulunamadı3)"  # Fallback if no valid response is found
def extract_numeric_response_choices(output_text):
    """
    Extract the numeric response from the output text.
    Assumes the output is a single number (e.g., "1", "2").
    """
    import re
    match = re.match(r'^[1-9]\d*$', output_text.strip())  # Matches a single number
    return match.group(0) if match else "(Yanıt bulunamadı4)"
def validate_response(numeric_response, response_options):
    """
    Validate the numeric response against the extracted options.
    If valid, map it back to the full option text (e.g., '1. Çok Önemli').
    """
    if numeric_response in response_options:
        # Find and return the matching full option (e.g., '1. Çok Önemli')
        for option in response_options:
            if option.startswith(numeric_response):
                return option
    return "(Yanıt bulunamadı2)"
def extract_response_after_cevap(raw_response):
    """
    Extract the numeric response immediately following the keyword 'Cevap:' in the raw response.

    Args:
        raw_response (str): The raw response generated by the model.

    Returns:
        str: The extracted numeric response (e.g., '1', '2', etc.), or a fallback if no valid response is found.
    """
    import re

    # Use regex to find the "Cevap:" keyword and the number following it
    match = re.search(r"Cevap:\s*(\d+)", raw_response)

    if match:
        return match.group(1)  # Return the numeric response (e.g., "1", "2")

    return "(Yanıt bulunamadı1)"  # Return a fallback if no valid match is found

"""AHMET'S NEW TRY:

"""

#final_prompts_df = pd.read_csv('/content/final_prompts.csv', encoding='utf-8', on_bad_lines='skip', lineterminator='\n')
final_prompts_df = pd.read_csv('/content/small_seperated_prompt.csv', encoding='utf-8', on_bad_lines='skip', lineterminator='\n')
import torch.nn.functional as F
import ast
import torch
import torch.nn.functional as F
import pandas as pd
# Function to extract token probabilities
def calculate_token_probabilities(logits, tokens_of_interest, tokenizer):
    # Convert tokens to IDs
    token_ids = [tokenizer.encode(token, add_special_tokens=False)[0] for token in tokens_of_interest]

    # Apply softmax to logits
    probabilities = torch.softmax(logits, dim=-1)

    # Retrieve probabilities for the tokens of interest
    token_probs = {token: probabilities[0, token_id].item() if token_id < probabilities.size(1) else 0.0
                   for token, token_id in zip(tokens_of_interest, token_ids)}

    return token_probs

def extract_response_after_cevap(raw_response):
    """
    Extract the response (numeric or letter) immediately following the keyword 'Cevap:' in the raw response.

    Args:
        raw_response (str): The raw response generated by the model.

    Returns:
        str: The extracted response (e.g., '1', '2', 'A', 'B', etc.), or a fallback if no valid response is found.
    """
    import re

    # Match numeric responses (e.g., '1', '2', '3')
    numeric_match = re.search(r"Cevap:\s*(\d+)", raw_response)

    if numeric_match:
        return numeric_match.group(1)  # Return the numeric response

    # Match letter responses (e.g., 'A', 'B', 'C', 'D')
    letter_match = re.search(r"Cevap:\s*([A-Da-d])", raw_response)

    if letter_match:
        return letter_match.group(1).upper()  # Normalize to uppercase letters

    return "(Yanıt bulunamadı)"  # Fallback if no valid response is found

#old
def create_prompts(persona_prompt, question_text, response_options):
    """
    Create prompts with numeric and letter-based options in both original and reversed orders,
    ensuring numbering remains ascending for all formats.
    """
    # Maintain ascending numbers but reverse the options
    options_numeric = {str(i + 1): option for i, option in enumerate(response_options)}
    options_reversed_numeric = {str(i + 1): option for i, option in enumerate(response_options[::-1])}
    options_letter = {chr(97 + i): option for i, option in enumerate(response_options)}
    options_reversed_letter = {chr(97 + i): option for i, option in enumerate(response_options[::-1])}

    def format_options(option_dict):
        """Format options as a string with the order preserved."""
        return "\n".join([f"{key}. {value}" for key, value in option_dict.items()])

    # Create prompts with both original and reversed orders
    prompts = {
        "Numeric Original": (
            f"{persona_prompt}\n\n{question_text}\n\nCevap seçenekleri:\n{format_options(options_numeric)}"
        ),
        "Numeric Reversed": (
            f"{persona_prompt}\n\n{question_text}\n\nCevap seçenekleri:\n{format_options(options_reversed_numeric)}"
        ),
        "Letter Original": (
            f"{persona_prompt}\n\n{question_text}\n\nCevap seçenekleri:\n{format_options(options_letter)}"
        ),
        "Letter Reversed": (
            f"{persona_prompt}\n\n{question_text}\n\nCevap seçenekleri:\n{format_options(options_reversed_letter)}"
        ),
    }

    # Tokens of interest corresponding to each prompt order
    token_mappers = {
        "Numeric Original": list(options_numeric.keys()),
        "Numeric Reversed": list(options_numeric.keys()),  # Numbers remain the same
        "Letter Original": list(options_letter.keys()),
        "Letter Reversed": list(options_letter.keys()),    # Letters remain the same
    }

    return prompts, token_mappers

def create_combined_prompt(persona_prompt, question_text, response_options, format_type="numeric", reversed_order=False):
    """
    Create a combined prompt based on the format type (numeric or letter) with optional reversed order.

    Args:
        persona_prompt (str): The persona description.
        question_text (str): The question to ask.
        response_options (list): The list of response options.
        format_type (str): Either 'numeric' or 'letter' to specify the response format.
        reversed_order (bool): Whether to reverse the order of the options.

    Returns:
        str, list: The combined prompt and corresponding tokens of interest.
    """
    # Reverse the options if specified
    if reversed_order:
        response_options = response_options[::-1]

    if format_type == "numeric":
        # Numeric format: 1. Option, 2. Option, etc.
        formatted_options = "\n".join([f"{i + 1}. {option}" for i, option in enumerate(response_options)])
        tokens_of_interest = [str(i + 1) for i in range(len(response_options))]
    elif format_type == "letter":
        # Letter format: A. Option, B. Option, etc.
        formatted_options = "\n".join([f"{chr(65 + i)}. {option}" for i, option in enumerate(response_options)])
        tokens_of_interest = [chr(65 + i) for i in range(len(response_options))]
    else:
        raise ValueError("Invalid format type. Use 'numeric' or 'letter'.")
    combined_prompt = (
        f"<bos><start_of_turn>user\n"
        "Verilen her soru için şu formatı takip et.\n\n"
        "[Soru]\n"
        "Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.\n\n"
        f"Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir {'harfle [harf]' if format_type == 'letter' else 'rakamla [sayı]'} formatında cevap verin. "
        f"Ayrıntılı bilgi vermeyin veya seçtiğiniz {'harften' if format_type == 'letter' else 'rakamdan'} başka herhangi bir bilgi vermeyin.\n\n"
        f"{persona_prompt}\n\n"
        "Bu kişi aşağıdaki soruya ne cevap verir?\n"
        f"{question_text}\n\n"
        f"Cevap seçenekleri:\n{formatted_options}\n\n"
        f"Yalnızca bir yanıt seçin {'(A, B, C, vb.):' if format_type == 'letter' else '(1, 2, 3, vb.):'}\n\n"
        "<end_of_turn><start_of_turn>model\n\nCevap:\n"
    )
        #f"Yalnızca bir yanıt seçin ({', '.join(tokens_of_interest)}):\n\n"

    return combined_prompt, tokens_of_interest

def extract_most_probable_response(response_options, softmax_output, reversed_order=False):
    """
    Extract the most probable response option based on the softmax output, with support for reversed order.

    Args:
        response_options (list): The list of response options (e.g., ["Hiç Önemli Değil", "Pek Önemli Değil", ...]).
        softmax_output (list): The softmax probabilities for each response option.
        reversed_order (bool): Whether the options are in reversed order.

    Returns:
        str: The response option with the highest probability.
    """
    # Reverse the response options if needed
    if reversed_order:
        response_options = response_options[::-1]

    # Find the index of the highest probability
    max_index = softmax_output.index(max(softmax_output))

    # Return the corresponding response option
    return response_options[max_index]

with open("output_combined.csv", 'w', encoding='utf-8') as f_out:
    f_out.write('Prompt,Format,Reversed,Validated Response,Most Probable Response Option,Token Probabilities,Softmax Output\n')  # Header
    i=0
    formats = [("numeric", False), ("numeric", True), ("letter", False), ("letter", True)]
    for _, row in final_prompts_df.iterrows():
        persona_prompt = row['Persona']
        question_text = row['Question']
        response_options = ast.literal_eval(row['Response Options'])

        for format_type, reversed_order in formats:
            # Create the combined prompt and tokens of interest
            prompt, tokens_of_interest = create_combined_prompt(
                persona_prompt, question_text, response_options, format_type, reversed_order
            )

            # Encode the prompt
            inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(model.device)

            # Generate the response
            outputs = model.generate(
                input_ids=inputs.input_ids,
                max_new_tokens=1,
                output_scores=True,
                return_dict_in_generate=True,
            )

            # Decode the generated token
            raw_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True).strip()
            validated_response = extract_response_after_cevap(raw_response)

            # Extract logits and calculate probabilities
            logits = outputs.scores[0].detach().to("cpu")
            probabilities = torch.softmax(logits, dim=-1)

            # Retrieve probabilities for tokens of interest
            token_probs = {
                token: probabilities[0, tokenizer.encode(token, add_special_tokens=False)[0]].item()
                for token in tokens_of_interest
            }

            # Debugging
            #print("Raw Probabilities for Tokens:", token_probs)
            token_probs = calculate_token_probabilities(logits, tokens_of_interest, tokenizer)

            # Normalize probabilities using softmax
            token_ids = [tokenizer.encode(token, add_special_tokens=False)[0] for token in tokens_of_interest]
            scores = [float(logits[0][token_id]) for token_id in token_ids if token_id < logits.size(1)]
            softmax_output = F.softmax(torch.tensor(scores), dim=0).tolist()

            most_probable_response = extract_most_probable_response(response_options, softmax_output, reversed_order)

            # Write results to CSV
            f_out.write(
                f'"{prompt}","{format_type}","{reversed_order}","{validated_response}","{most_probable_response}","{token_probs}","{[round(prob, 3) for prob in softmax_output]}"\n'
            )
            if i<4:
              print(f"Raw Response: {raw_response}")
              print(f"Tokens of Interest: {tokens_of_interest}")
              print(f"Softmax Output: {softmax_output}")
              # Debugging output
              print(f"Processed Format: {format_type} | Reversed: {reversed_order}")
              print(f"Prompt: {prompt[:50]}...")
              print(f"Validated Response: {validated_response}")
              print("Token Probabilities:", token_probs)
              print("Softmax Output:", [round(prob, 3) for prob in softmax_output])
              print("-" * 50)

              i += 1



print("Responses generated and saved to 'output_combined.csv'")

"""OLD WORKING VERSION:"""

final_prompts_df = pd.read_csv('mixed_final.csv', encoding='utf-8', on_bad_lines='skip', lineterminator='\n')
import torch.nn.functional as F

# Function to extract token probabilities
def calculate_token_probabilities(logits, tokens_of_interest, tokenizer):
    # Convert tokens to IDs
    token_ids = [tokenizer.encode(token, add_special_tokens=False)[0] for token in tokens_of_interest]

    # Apply softmax to logits
    probabilities = torch.softmax(logits, dim=-1)

    # Retrieve probabilities for the tokens of interest
    token_probs = {token: probabilities[0, token_id].item() if token_id < probabilities.size(1) else 0.0
                   for token, token_id in zip(tokens_of_interest, token_ids)}

    return token_probs

# Process each prompt
with open("output.csv", 'w', encoding='utf-8') as f_out:
    f_out.write('Prompt,Validated Response,Token Probabilities\n')  # Write header

    for _, row in final_prompts_df.iterrows():
        prompt = row['Final Prompt']
        #prompt = prompt_try
        # Combine prompt with chat template
        combined_prompt = (
            "<bos><start_of_turn>user\n"
            "Verilen her soru için şu formatı takip et.\n\n"
            "[Soru]\n"
            "Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.\n\n"
            "Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir rakamla [sayı] formatında cevap verin. "
            "Ayrıntılı bilgi vermeyin veya seçtiğiniz rakamdan başka herhangi bir bilgi vermeyin.\n\n"
            f"{prompt}\n\n<end_of_turn><start_of_turn>model\n\nCevap: "
        )

        # Encode the prompt
        inputs = tokenizer(combined_prompt, return_tensors="pt", add_special_tokens=False).to(model.device)

        # Generate the response
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=1,
            output_scores=True,
            return_dict_in_generate=True,
        )

        # Decode the generated token
        raw_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True).strip()
        #print("raw_response",raw_response)
        # Validate and extract the numeric response
        #validated_response = extract_numeric_response(raw_response)
        validated_response = extract_response_after_cevap(raw_response)
        #print(prompt)
        # Extract logits and calculate probabilities
        logits = outputs.scores[0]
        #tokens_of_interest = ["1", "2", "3", "4","5", "6", "7", "8","9", "10", "11"]
        tokens_of_interest = extract_response_options(prompt)
        token_probs = calculate_token_probabilities(logits, tokens_of_interest, tokenizer)
        a = outputs.scores[0].detach().to("cpu")
        t = []
        for i in range(1,5):
            t.append(float(a[0][tokenizer.encode(str(i), add_special_tokens=False)]))
        softmax_output = F.softmax(torch.tensor(t), dim=0)
        softmax_output = [round(float(x),3) for x in softmax_output]
        all_probs = F.softmax(a)
        non_normalized = []
        print(tokens_of_interest)
        for i in tokens_of_interest:
            non_normalized.append(float(all_probs[0][tokenizer.encode(str(i), add_special_tokens=False)]))
        # Print the result

        # Write results to CSV
        f_out.write(f'"{prompt}","{validated_response}","{token_probs}"\n')
        print(softmax_output)

        # Debugging output
        print(f"Processed Prompt: {prompt[:50]}...")
        print(f"Validated Response: {validated_response}")
        print("Token Probabilities:", token_probs)

print(f"Responses generated and saved to")

# Function to extract token probabilities
def calculate_token_probabilities(logits, tokens_of_interest, tokenizer):
    # Convert tokens to IDs
    token_ids = [tokenizer.encode(token, add_special_tokens=False)[0] for token in tokens_of_interest]

    # Apply softmax to logits
    probabilities = torch.softmax(logits, dim=-1)

    # Retrieve probabilities for the tokens of interest
    token_probs = {token: probabilities[0, token_id].item() if token_id < probabilities.size(1) else 0.0
                   for token, token_id in zip(tokens_of_interest, token_ids)}

    return token_probs

# Process each prompt
with open("output.csv", 'w', encoding='utf-8') as f_out:
    f_out.write('Prompt,Validated Response,Token Probabilities\n')  # Write header

    for _, row in final_prompts_df.iterrows():
        prompt = row['Final Prompt']
        #prompt = prompt_try
        # Combine prompt with chat template
        combined_prompt = (
            "<bos><start_of_turn>user\n"
            "Verilen her soru için şu formatı takip et.\n\n"
            "[Soru]\n"
            "Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.\n\n"
            "Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir rakamla [sayı] formatında cevap verin. "
            "Ayrıntılı bilgi vermeyin veya seçtiğiniz rakamdan başka herhangi bir bilgi vermeyin.\n\n"
            f"{prompt}\n\n<end_of_turn><start_of_turn>model\n\nCevap: "
        )

        # Encode the prompt
        inputs = tokenizer(combined_prompt, return_tensors="pt", add_special_tokens=False).to(model.device)

        # Generate the response
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=1,
            output_scores=True,
            return_dict_in_generate=True,
        )

        # Decode the generated token
        raw_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True).strip()
        #print("raw_response",raw_response)
        # Validate and extract the numeric response
        #validated_response = extract_numeric_response(raw_response)
        validated_response = extract_response_after_cevap(raw_response)
        #print(prompt)
        # Extract logits and calculate probabilities
        logits = outputs.scores[0]
        #tokens_of_interest = ["1", "2", "3", "4","5", "6", "7", "8","9", "10", "11"]
        tokens_of_interest = extract_response_options(prompt)
        all_probs = F.softmax(logits, dim=1)  # Add dim for explicit softmax

        token_probs = {}
        for token in tokens_of_interest:
            token_id = tokenizer.encode(token, add_special_tokens=False)[0]
            token_probs[token] = all_probs[0, token_id].item()

        # Normalize token probabilities
        t = [token_probs.get(token, 0.0) for token in tokens_of_interest]
        softmax_output = F.softmax(torch.tensor(t), dim=0).tolist()
        print(softmax_output)
        print([round(prob, 3) for prob in t])
        # Debugging output
        print(f"Processed Prompt: {prompt[:50]}...")
        print(f"Validated Response: {validated_response}")
        print("Token Probabilities:", token_probs)
print(f"Responses generated and saved to")

print(raw_response)

combined_prompt1 = """<bos><start_of_turn>user
Verilen her soru için şu formatı takip et.

[Soru]
Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.

Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir harfle [harf] formatında cevap verin. Ayrıntılı bilgi vermeyin veya seçtiğiniz harften başka herhangi bir bilgi vermeyin.

Ahmet, 24 yaşında bekâr bir erkek, çocuğu yok, hiç okula gitmemiş, Ankara şehrinde kent merkezinde yaşayan, kendi sosyal sınıfını alt sınıf olarak tanımlayan, işsiz/iş arayan birisidir.

Bu kişi aşağıdaki soruya ne cevap verir?
Şimdi soracaklarımdan her biri sizin için ne kadar önemlidir? Yani bunlar sizin için çok mu önemli, biraz mı önemli, pek önemli değil mi, yoksa hiç önemi yok mu? Aileniz

Cevap seçenekleri:
A. Hiç Önemli Değil
B. Pek Önemli Değil
C. Biraz Önemli
D. Çok Önemli

Yalnızca bir yanıt seçin (A, B, C, vb.):

<end_of_turn><start_of_turn>model

Cevap:

"""

combined_prompt2 = """<bos><start_of_turn>user
Verilen her soru için şu formatı takip et.

[Soru]
Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.

Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir harfle [harf] formatında cevap verin. Ayrıntılı bilgi vermeyin veya seçtiğiniz harften başka herhangi bir bilgi vermeyin.

Ahmet, 24 yaşında bekâr bir erkek, çocuğu yok, hiç okula gitmemiş, Ankara şehrinde kent merkezinde yaşayan, kendi sosyal sınıfını alt sınıf olarak tanımlayan, işsiz/iş arayan birisidir.

Bu kişi aşağıdaki soruya ne cevap verir?
Şimdi soracaklarımdan her biri sizin için ne kadar önemlidir? Yani bunlar sizin için çok mu önemli, biraz mı önemli, pek önemli değil mi, yoksa hiç önemi yok mu? Aileniz

Cevap seçenekleri:
A. Çok Önemli
B. Biraz Önemli
C. Pek Önemli Değil
D. Hiç Önemli Değil

Yalnızca bir yanıt seçin (A, B, C, vb.):

<end_of_turn><start_of_turn>model

Cevap:

"""

combined_prompt_num1 = """<bos><start_of_turn>user
Verilen her soru için şu formatı takip et.

[Soru]
Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.

Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir rakamla [sayı] formatında cevap verin. Ayrıntılı bilgi vermeyin veya seçtiğiniz rakamdan başka herhangi bir bilgi vermeyin.

Ahmet, 24 yaşında bekâr bir erkek, çocuğu yok, hiç okula gitmemiş, Ankara şehrinde kent merkezinde yaşayan, kendi sosyal sınıfını alt sınıf olarak tanımlayan, işsiz/iş arayan birisidir.

Bu kişi aşağıdaki soruya ne cevap verir?
Şimdi soracaklarımdan her biri sizin için ne kadar önemlidir? Yani bunlar sizin için çok mu önemli, biraz mı önemli, pek önemli değil mi, yoksa hiç önemi yok mu? Aileniz

Cevap seçenekleri:
1. Hiç Önemli Değil
2. Pek Önemli Değil
3. Biraz Önemli
4. Çok Önemli

Yalnızca bir yanıt seçin (1, 2, 3, vb.):

<end_of_turn><start_of_turn>model

Cevap:

"""

combined_prompt_num2 = """<bos><start_of_turn>user
Verilen her soru için şu formatı takip et.

[Soru]
Size bir kişinin tanımı ve bir soru verilir. Soru metnindeki seçeneklerden en uygun yanıtı seçmelisiniz.

Lütfen yanıt seçeneklerinden yalnızca birini kullanarak yanıtlayın ve tek bir rakamla [sayı] formatında cevap verin. Ayrıntılı bilgi vermeyin veya seçtiğiniz rakamdan başka herhangi bir bilgi vermeyin.

Ahmet, 24 yaşında bekâr bir erkek, çocuğu yok, hiç okula gitmemiş, Ankara şehrinde kent merkezinde yaşayan, kendi sosyal sınıfını alt sınıf olarak tanımlayan, işsiz/iş arayan birisidir.

Bu kişi aşağıdaki soruya ne cevap verir?
Şimdi soracaklarımdan her biri sizin için ne kadar önemlidir? Yani bunlar sizin için çok mu önemli, biraz mı önemli, pek önemli değil mi, yoksa hiç önemi yok mu? Aileniz

Cevap seçenekleri:
1. Çok Önemli
2. Biraz Önemli
3. Pek Önemli Değil
4. Hiç Önemli Değil

Yalnızca bir yanıt seçin (1, 2, 3, vb.):

<end_of_turn><start_of_turn>model

Cevap:

"""

for p in [combined_prompt1, combined_prompt2, combined_prompt_num1, combined_prompt_num2]:
    model.eval()
    with torch.no_grad():
        # Encode the prompt
        inputs = tokenizer(p, return_tensors="pt", add_special_tokens=False).to(model.device)

        # Generate the response
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=1,
            output_scores=True,
            return_dict_in_generate=True,
        )

        # Decode the generated token
        raw_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True).strip()
        #print("raw_response",raw_response)
        # Validate and extract the numeric response
        #validated_response = extract_numeric_response(raw_response)
        validated_response = extract_response_after_cevap(raw_response)

        # Extract logits and calculate probabilities
        logits = outputs.scores[0]
        #tokens_of_interest = ["1", "2", "3", "4","5", "6", "7", "8","9", "10", "11"]
        tokens_of_interest = extract_numeric_response(p)
        token_probs = calculate_token_probabilities(logits, tokens_of_interest, tokenizer)
        a = outputs.scores[0].detach().to("cpu")
        t = []
        mapper1 = {1:"A", 2:"B", 3:"C", 4:"D"}
        mapper2 = {1:"1", 2:"2", 3:"3", 4:"4"}
        mapper = mapper1
        if p==combined_prompt1 or p==combined_prompt2:
            mapper = mapper1
        else:
            mapper = mapper2
        for i in range(1,5):

            t.append(float(a[0][tokenizer.encode(mapper[i], add_special_tokens=False)]))
        softmax_output = F.softmax(torch.tensor(t), dim=0)
        softmax_output = [round(float(x),3) for x in softmax_output]

        all_probs = F.softmax(a)
        non_normalized = []
        for i in range(1,5):
            non_normalized.append(float(all_probs[0][tokenizer.encode(mapper[i], add_special_tokens=False)]))

        # Print the result
        if p==combined_prompt1 or p==combined_prompt_num1:
            print("Reverse", softmax_output)
            print([round(float(x),3) for x in non_normalized])
        else:
            print("Normal", softmax_output)
            print([round(float(x),3) for x in non_normalized])

all_probs.shape



import torch
import torch.nn.functional as F



"""ahmet:"""



for p in [combined_prompt1, combined_prompt2, combined_prompt_num1, combined_prompt_num2]:
    model.eval()
    with torch.no_grad():
        # Encode the prompt
        inputs = tokenizer(p, return_tensors="pt", add_special_tokens=False).to(model.device)

        # Generate the response
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=1,
            output_scores=True,
            return_dict_in_generate=True,
        )

        # Decode the generated token
        raw_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True).strip()

        # Extract logits and calculate probabilities
        logits = outputs.scores[0]
        all_probs = F.softmax(logits, dim=1)  # Add dim for explicit softmax

        # Extract tokens of interest
        if p in [combined_prompt1, combined_prompt2]:
            tokens_of_interest = ["A", "B", "C", "D"]
        else:
            tokens_of_interest = ["1", "2", "3", "4"]

        token_probs = {}
        for token in tokens_of_interest:
            token_id = tokenizer.encode(token, add_special_tokens=False)[0]
            token_probs[token] = all_probs[0, token_id].item()

        # Normalize token probabilities
        t = [token_probs.get(token, 0.0) for token in tokens_of_interest]
        softmax_output = F.softmax(torch.tensor(t), dim=0).tolist()

        # Debugging Output
        print(f"Prompt Type: {'Reverse' if p in [combined_prompt1, combined_prompt_num1] else 'Normal'}")
        print("Normalized Probabilities:", [round(prob, 3) for prob in softmax_output])
        print("Non-normalized Probabilities:", [round(prob, 3) for prob in t])



